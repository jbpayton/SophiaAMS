# SophiaAMS
**Associative Semantic Memory System with REST API**

## Overview

SophiaAMS is an intelligent memory system for LLM-based applications featuring conversation processing, document ingestion, and semantic retrieval. It transforms conversations and documents into a knowledge graph of semantic triples, enabling natural memory-aware AI interactions.

## âœ¨ Key Features

- **ğŸ—¨ï¸ Conversational Memory**: Automatically processes chat conversations into semantic knowledge
- **ğŸ“„ Document Processing**: Ingests text files, web content, and documents into memory
- **ğŸ§  Semantic Retrieval**: Finds relevant memories using vector similarity and topic matching
- **ğŸŒ REST API**: FastAPI server with comprehensive endpoints for integration
- **ğŸ’» Interactive Client**: Streamlit-based chat interface with memory visualization
- **ğŸ“Š Knowledge Exploration**: Browse topics, entities, and relationships in your knowledge graph
- **âš¡ Smart Buffering**: Server-side conversation batching for optimal performance

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/jbpayton/SophiaAMS.git
cd SophiaAMS

# Install dependencies
pip install -r requirements.txt
pip install -r requirements_api.txt

# Download spaCy model
python -m spacy download en_core_web_sm
```

### 2. Environment Setup

Create a `.env` file with your LLM configuration:

```bash
LLM_API_BASE=http://your-llm-server:1234/v1
LLM_API_KEY=your-api-key
EXTRACTION_MODEL=your-model-name
```

### 3. Launch the System

```bash
# Start the API server
python api_server.py

# In another terminal, start the Streamlit client
streamlit run streamlit_client.py
```

Open http://localhost:8501 in your browser to access the interactive interface!

## ğŸ—ï¸ Architecture

### Components

- **API Server** (`api_server.py`): FastAPI REST endpoints for memory operations
- **Streamlit Client** (`streamlit_client.py`): Interactive web interface for chat and exploration
- **AssociativeSemanticMemory**: Core memory processing with LLM integration
- **VectorKnowledgeGraph**: Qdrant-powered vector storage for semantic triples
- **ConversationProcessor**: Transforms chat messages into knowledge triples

### Data Flow

```
Chat Input â†’ Memory Retrieval â†’ LLM Response â†’ Knowledge Storage
     â†‘                â†“                 â†“              â†“
User Interface â† Memory Context â† Response Gen. â†’ Triple Extraction
```

## ğŸ’¬ Usage Examples

### Interactive Chat Interface

The Streamlit client provides:
- **Real-time chat** with memory-aware responses
- **Memory visualization** showing retrieved context
- **File upload** for document ingestion
- **Knowledge exploration** of topics and entities

### REST API Usage

```python
import requests

# Query memory
response = requests.post("http://localhost:8000/query", json={
    "text": "What did we discuss about AI?",
    "limit": 10,
    "return_summary": True
})

# Ingest conversation
response = requests.post("http://localhost:8000/ingest/conversation", json={
    "messages": [
        {"role": "user", "content": "I love machine learning"},
        {"role": "assistant", "content": "That's great! What aspects interest you most?"}
    ],
    "session_id": "user123"
})

# Upload document
response = requests.post("http://localhost:8000/ingest/document", json={
    "text": "Your document content here...",
    "source": "research_paper.txt"
})
```

### Programmatic Usage

```python
from AssociativeSemanticMemory import AssociativeSemanticMemory
from VectorKnowledgeGraph import VectorKnowledgeGraph
from ConversationProcessor import ConversationProcessor

# Initialize system
kgraph = VectorKnowledgeGraph()
memory = AssociativeSemanticMemory(kgraph)
processor = ConversationProcessor(memory)

# Process conversation
messages = [
    {"role": "user", "content": "Tell me about quantum computing"},
    {"role": "assistant", "content": "Quantum computing uses quantum mechanics..."}
]

result = processor.process_conversation(messages, entity_name="Assistant")

# Query related information
info = memory.query_related_information("quantum computing applications")
print(info["summary"])
```

## ğŸŒ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Server health check |
| `/stats` | GET | Knowledge graph statistics |
| `/query` | POST | Search memory for relevant information |
| `/ingest/conversation` | POST | Process conversation messages |
| `/ingest/document` | POST | Add document to memory |
| `/explore/topics` | GET | Browse knowledge topics |
| `/explore/entities` | GET | View connected entities |
| `/conversation/buffer/{session_id}` | GET | Check conversation buffer status |

See [API_README.md](API_README.md) for detailed API documentation.

## ğŸ”§ Configuration

### Environment Variables

- `LLM_API_BASE`: Base URL for your LLM API
- `LLM_API_KEY`: Authentication key for LLM access
- `EXTRACTION_MODEL`: Model name for triple extraction and summarization

### Memory Settings

Configure in your initialization code:

```python
# Conversation buffer settings (in api_server.py)
BUFFER_SIZE = 5        # Process every N messages
MIN_BUFFER_TIME = 30   # Or every 30 seconds

# Query limits
DEFAULT_LIMIT = 10     # Default retrieval limit
CHAT_LIMIT = 10        # Auto-retrieval in chat
```

## ğŸ“Š Knowledge Graph

SophiaAMS represents knowledge as semantic triples:

```
(Subject, Predicate, Object)
("User", "likes", "machine learning")
("Python", "is_used_for", "AI development")
("Assistant", "explained", "neural networks")
```

Each triple includes:
- **Vector embeddings** for semantic similarity
- **Topics** for categorical organization
- **Metadata** (timestamps, sources, confidence scores)
- **Speaker attribution** for conversation context

## ğŸ” Memory Retrieval

The system uses multiple retrieval strategies:

1. **Text Similarity**: Vector search against stored content
2. **Topic Matching**: Keyword-based topic filtering
3. **Associative Expansion**: Multi-hop relationship traversal
4. **LLM Summarization**: Intelligent summary generation

## ğŸ› ï¸ Development

### Project Structure

```
SophiaAMS/
â”œâ”€â”€ api_server.py              # FastAPI REST server
â”œâ”€â”€ streamlit_client.py        # Interactive web interface
â”œâ”€â”€ AssociativeSemanticMemory.py  # Core memory system
â”œâ”€â”€ VectorKnowledgeGraph.py    # Vector storage layer
â”œâ”€â”€ ConversationProcessor.py   # Chat message processing
â”œâ”€â”€ MemoryExplorer.py         # Knowledge graph exploration
â”œâ”€â”€ tests/                    # Test suite
â””â”€â”€ docs/                     # Additional documentation
```

### Testing

```bash
# Run test suite
python -m pytest tests/

# Test specific components
python -m pytest tests/test_conversation_processor.py
```

## ğŸ“š Dependencies

### Core Dependencies
- `fastapi`: REST API framework
- `streamlit`: Interactive web interface
- `sentence-transformers`: Embedding generation
- `qdrant-client`: Vector database
- `openai`: LLM integration
- `spacy`: Natural language processing

### Optional Dependencies
- `trafilatura`: Web content extraction
- `tiktoken`: Token counting and chunking
- `networkx`: Graph analysis
- `matplotlib`: Visualization

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/jbpayton/SophiaAMS/issues)
- **Documentation**: [API_README.md](API_README.md)
- **Development Notes**: [DEVELOPMENT_NOTES.md](DEVELOPMENT_NOTES.md)

---

*SophiaAMS - Intelligent memory for the age of AI* ğŸ§ âœ¨